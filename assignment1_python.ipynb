{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Simple Hand-written Classifier\n",
    "### ⭐인천대학교 임베디드시스템공학과 2023학년도 인공지능 교과목 과제 1번⭐\n",
    "\n",
    "\n",
    "목표 : 자신이 만든 손글씨 이미지(t, u, v, w, x, y, z)를 학습하여 인식하는 모델 만들기  \n",
    "\n",
    "Model 클래스를 구현하여 학습에 필요한 중요 변수들을 보관하였으며, 각 Model 클래스는  \n",
    "Layer 클래스를 Property로 가져, 각 레이어에 필요한 정보들을 보관하였습니다.  \n",
    "Layer의 객체는 Model 클래스 객체의 Property인 layers에 존재합니다.  \n",
    "  \n",
    "* 현재 설정된 모델  \n",
    "Layer의 개수 : 4개  \n",
    "각 Layer별 노드 개수 : 256(input), 96(hidden), 48(hidden), 7(output)  \n",
    "=> 모든 레이어는 FC(Fully Connected Layer) Layer이다.  \n",
    "  \n",
    "코드의 처음 부분에서는 주어진 Path에서 Train(420개) Test(140개)를 Load하여 저장합니다.  \n",
    "불러온 데이터는 Model Class의 객체에 저장됩니다.  \n",
    "이 때 데이터를 가져오는 과정에서 모든 Pixel 값을 255로 나눠 Normalize합니다.  \n",
    "또한 Label도 One hot encoding으로 만들어서 Model Class 객체에 함께 저장해 둡니다.\n",
    "  \n",
    "이후에는 하나의 Example씩 For loop을 돌면서 Forward Pass/Backward Pass를 진행합니다.  \n",
    "=> 단, Parameter Update는 Batch Gradient Descent로 이루어진다. (Mini-batch는 사용하지 않음.)\n",
    "\n",
    "Input은 16*16의 Image를 Flatten(256개의 값을 갖는 Vector)한 Vector이며,  \n",
    "각 레이어 별로 다음 연산을 거칩니다. (Input Layer는 값을 넣어주기만 하는 역할)  \n",
    "  \n",
    "1) Z = WX + b (Input X에 가중치 Matrix W를 곱하고, bias b를 더해준다.)  \n",
    "2) a = Activation(Z) (구해진 Z값을 activation function에 넣어 activation을 얻는다).  \n",
    "=> hidden : ReLU 사용  \n",
    "=> output : Softmax 사용  \n",
    "  \n",
    "이 과정을 거치면 최종적으로 (7, 1) size의 output이 나오게 되며, Softmax를 거쳤기 때문에  \n",
    "각각의 element의 값이 0~1이고, 합이 1인 확률값으로 표현됩니다.  \n",
    "이 값을 One hot encoding된 label과 비교하여 정답률을 계산합니다.    \n",
    "\n",
    "본 코드는 Epoch, Learning Rate, Layer의 개수, Layer별 Node의 개수 등과 같은 Hyperparameter를 바꿔가면서  \n",
    "학습을 진행해 볼 수 있도록 작성되었습니다. (해당 부분이 위치한 각 Cell별 설명에 작성하였음.)  \n",
    "<br>\n",
    "\n",
    "* 현재 설정한 모델의 구조  \n",
    "    현재 모델의 레이어는 네 개이며, 각 레이어의 노드 개수는 [256, 96, 48, 7]입니다.  \n",
    "    Epoch은 200, Learning Rate는 0.03이며, Training Data/Test Data는 420/140개 입니다.  \n",
    "    이 설정을 바탕으로 한 개의 exmaple에 대한 forward pass/backward pass를 수행하고,  \n",
    "    이 과정을 전체 example에 대해서 반복한 뒤에 Batch Gradient Descent로 parameter를 업데이트 합니다.  \n",
    "    그리고 이 과정을 epoch 수만큼 반복한 뒤에, Test Data에 대한 inference 결과를 얻어냅니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import math\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Module Import__  (⬆️)\n",
    "\n",
    "프로젝트에 필요한 Module을 Import 합니다.  \n",
    "1) Image : png 이미지를 load하고 데이터 값을 읽어오는데 사용됩니다.  \n",
    "2) math : 수학적 연산을 표현하는데 사용합니다.  \n",
    "3) random : Parameter를 Initialization할 때 사용합니다.  \n",
    "4) os : file을 읽을 때, 해당 파일의 경로를 표현하는데 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_vector(target_type):\n",
    "    '''\n",
    "        입력받은 데이터에 맞는 one hot encoded label을 만들어 반환하는 함수\n",
    "    '''\n",
    "    \n",
    "    # file에서 얻어낸 target_value에 따라서\n",
    "    # 매칭되는 target_vector를 return한다.\n",
    "    if \"t\" in target_type:\n",
    "        return [1, 0, 0, 0, 0, 0, 0]\n",
    "    elif \"u\" in target_type:\n",
    "        return [0, 1, 0, 0, 0, 0, 0]\n",
    "    elif \"v\" in target_type:\n",
    "        return [0, 0, 1, 0, 0, 0, 0]\n",
    "    elif \"w\" in target_type:\n",
    "        return [0, 0, 0, 1, 0, 0, 0]\n",
    "    elif \"x\" in target_type:\n",
    "        return [0, 0, 0, 0, 1, 0, 0]\n",
    "    elif \"y\" in target_type:\n",
    "        return [0, 0, 0, 0, 0, 1, 0]\n",
    "    elif \"z\" in target_type:\n",
    "        return [0, 0, 0, 0, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __get_target_vector()__ (⬆️)\n",
    "\n",
    "Train/Test 데이터를 load할 때, 읽어낸 데이터 파일의 이름에서 target label의 type을 분리합니다.  \n",
    "이 값을 get_target_vector() 함수에 입력하면 one hot encoding된 target label vector를 반환합니다.  \n",
    "(t, u, v, w, x, y, z) 7개의 label type이 존재하므로, 7개의 type의 vector를 반환합니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_img_pixels(path, data_type):\n",
    "    '''\n",
    "        주어진 path에 들어있는 학습 데이터를 읽고,\n",
    "        RGB pixel value를 저장하는 함수\n",
    "    '''\n",
    "    \n",
    "    loaded_data = []\n",
    "    target = []\n",
    "    \n",
    "    for file_name in os.listdir(path):\n",
    "        img = Image.open(path+file_name)\n",
    "        img_data = img.getdata()\n",
    "        \n",
    "        target_value = file_name.split('_')\n",
    "        \n",
    "        if (data_type == \"train\"):\n",
    "            # target_value 저장\n",
    "            target_value = target_value[0]\n",
    "        elif(data_type == \"test\"):\n",
    "            target_value = target_value[1]\n",
    "        \n",
    "        # target의 type(t, u, v, w, x, y, z)에 따른 label을 반환하고, \n",
    "        # 이를 data set의 전체 target을 보관하는 list에 저장한다.\n",
    "        target_vector = get_target_vector(target_value)\n",
    "        target.append(target_vector)\n",
    "        \n",
    "        pixel_values = [] # 하나의 img를 구성하는 pixel 값을 보관하는 list\n",
    "        img_len = len(img_data) # img example의 길이(16*16 = 256)\n",
    "        \n",
    "        # 하나의 이미지에 들어있는 모든 픽셀에 대해서 loop 동작\n",
    "        for pixel_index in range(img_len):\n",
    "            \n",
    "            # 각 pixel 값을 255로 나눠서 float로 변환해 표현력을 증가시키고,\n",
    "            # 값의 범위를 0~1 사이로 줄여서 forward pass 과정에서 곱해지는\n",
    "            # 값의 크기에 의한 영향을 줄인다. (값의 절댓값이 클수록 activation이 커지는 현상을 감소)\n",
    "            pixel_values.append(img_data[pixel_index]/255)\n",
    "        \n",
    "        # 하나의 example의 모든 pixel 값을 보관한 pixel_values를 loaded_data에 추가한다.\n",
    "        # 따라서 모든 for loop을 돌면, loaded_data는 train or test의 모든 data set을 가지게 될 것이다.\n",
    "        loaded_data.append(pixel_values) \n",
    "    \n",
    "    return loaded_data, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __parse_image_pixels()__ (⬆️)\n",
    "\n",
    "인자로 전달되는 path(image의 경로 + 이름)와 train/test 여부에 따라 이미지 데이터로부터 값을 읽는 함수입니다.  \n",
    "읽어낸 이미지에서 data와 label을 분리하고, data를 255로 나눠줍니다. (Normalization)  \n",
    "  \n",
    "data를 255로 나누는 것은 pixel 값의 범위가 0~255이기 때문인데, 간단하게 255로 값을 나눠줌으로써  \n",
    "float로 변환하여 표현력을 증가시키고, 값의 절댓값에 의해 activation이 커지는 문제를 완화합니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(input):\n",
    "    return max(input, 0)\n",
    "\n",
    "def element_wise_relu(input):\n",
    "    '''\n",
    "        hidden layer에 사용 될 activation function(ReLU)\n",
    "        input vector를 전달받아서 각각에 대해 relu를 적용한 뒤 결과를 반환한다.\n",
    "    '''\n",
    "    \n",
    "    activation_len = len(input)\n",
    "    \n",
    "    for activation_idx in range(activation_len):\n",
    "        input[activation_idx] = relu(input[activation_idx])\n",
    "    \n",
    "    return input\n",
    "\n",
    "def softmax(input):\n",
    "    '''\n",
    "        output layer에 사용 될 activation function(Softmax)\n",
    "        t, u, v, w, x, y, z 각각에 대해서 정답일 확률을 출력한다.\n",
    "    '''\n",
    "    \n",
    "    total_exp_value_sum = 0\n",
    "    activation = []\n",
    "    \n",
    "    max_value = max(input)\n",
    "    \n",
    "    for idx in range(len(input)):\n",
    "        exp_value = math.exp(input[idx] - max_value)\n",
    "        total_exp_value_sum += exp_value\n",
    "        activation.append(exp_value)\n",
    "    \n",
    "    for idx in range(len(input)):\n",
    "        activation_value = activation[idx] / total_exp_value_sum\n",
    "        activation[idx] = activation_value\n",
    "    \n",
    "    return activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __activation functions__ (⬆️)\n",
    "\n",
    "Model의 Layer를 구성하는데 사용되는 Acitvation Function을 구현한 Cell입니다.  \n",
    "Hidden Layer에서는 ReLU를 사용하고, Output Layer에서는 Softmax를 사용합니다.  \n",
    "1) ReLU : input이 0보다 작거나 같으면 0을 출력, 양수라면 값을 그대로 출력합니다.\n",
    "2) Softmax : Multiple Classification에 사용되며, 현재 모델에서는 (7,1)의 결과를 출력합니다.  \n",
    "각 값이 0 ~ 1 사이의 확률값으로 표현되며, 전체 합은 1이기 때문에 결과적으로 (t, u, v, w, x, y, z) 중  \n",
    "하나의 값을 선택하는 역할을 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(prev_node_num, cur_node_num, weight, data):\n",
    "    '''\n",
    "        Forward Pass 과정에서, 각 Layer에서 행렬곱 함수\n",
    "    '''\n",
    "\n",
    "    output = []\n",
    "    \n",
    "    for i in range(cur_node_num):\n",
    "        value = 0\n",
    "        for j in range(prev_node_num):\n",
    "            value += weight[i][j] * data[j]\n",
    "        output.append(value)\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __dot_product()__ (⬆️)\n",
    "\n",
    "forward pass 과정에서 Z = WX + b 연산을 진행하는데 필요한 행렬곱 연산을 수행하는 함수입니다.  \n",
    "이전 노드의 개수와 현재 노드의 개수를 고려하면 weight와 bias, 그리고 data(Input)의 길이를 알 수 있으므로,  \n",
    "이 값들을 전달받아 이중 for loop을 통해 행렬곱을 진행합니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params_by_type(cur_layer_node_num, prev_layer_node_num, init_type):\n",
    "    ''' \n",
    "        Layer의 weight를 초기화하는 함수\n",
    "        init_type에 따라 Xavier 또는 He Initialization을 사용한다.\n",
    "    '''\n",
    "    \n",
    "    weight = []\n",
    "    bias = []\n",
    "    \n",
    "    dW = []\n",
    "    db = []\n",
    "    \n",
    "    std = 0\n",
    "    \n",
    "    if (init_type == \"xavier\"):\n",
    "        std = math.sqrt(1.0 / prev_layer_node_num)  \n",
    "    elif (init_type == \"he\"):\n",
    "        std = math.sqrt(2.0 / prev_layer_node_num)\n",
    "    \n",
    "    for cur_node_idx in range(cur_layer_node_num):\n",
    "        # bias는 모두 0으로 초기화한다.\n",
    "        # 또한, 학습에 사용할 gradient를 저장할 db는 bias와 똑같은 크기이므로,\n",
    "        # 이와 같은 크기로 모든 원소를 0을 가지도록 초기화한다.\n",
    "        bias.append(0)\n",
    "        db.append(0)\n",
    "        \n",
    "        # weight는 Xavier Initialization을 사용.\n",
    "        # gradient를 저장할 dW도 weight와 똑같은 크기로\n",
    "        # 모든 원소를 0을 가지도록 초기화한다.\n",
    "        weight_row = []\n",
    "        dW_weight_row = []\n",
    "        \n",
    "        for prev_node_idx in range(prev_layer_node_num):\n",
    "            # 평균은 0이고 표준편차는 std인 분포에서 random값을 선택한다.\n",
    "            random_number = random.gauss(0, std) \n",
    "            \n",
    "            weight_row.append(random_number) # 선택한 random값을 weight_row에 추가\n",
    "            dW_weight_row.append(0) # dW_weight_row는 gradient를 보관하므로, 처음에는 0으로 초기화\n",
    "            \n",
    "        weight.append(weight_row) # weight_row를 weight에 추가\n",
    "        dW.append(dW_weight_row) # dW_weight_row를 dW에 추가\n",
    "    \n",
    "    return weight, bias, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __init_params_by_type()__  \n",
    "  \n",
    "현재 Layer와 이전 Layer의 Node 개수를 전달받고,이 크기를 바탕으로  \n",
    "Weight, bias를 Random Initialize 합니다.\n",
    "  \n",
    "이 때, ReLU를 사용하는 Layer(Hidden Layer)는 He Initialization을 사용하고,  \n",
    "Softmax를 사용하는 Layer(Output Layer)는 Xavier Initialization을 사용합니다.\n",
    "\n",
    "(Xavier Initialization의 경우 실제로는 LeCun이지만 참고한 교재에서  \n",
    "Xavier Initialization이라고 지칭하여, 현재 프로젝트에서도  \n",
    "Xavier Initialization이라고 지정해 두었습니다.)\n",
    "  \n",
    "또, 각 Layer에서 Backward pass에서 gradient를 저장할 리스트가 필요하기 때문에,  \n",
    "이 과정에서 Weight와 bias의 크기와 똑같이 dW, db를 모두 0으로 초기화합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델을 구성하는 전체 Class 선언\n",
    "  \n",
    "1) *__Layer Class__*  \n",
    "Model을 구성하는 __Layer Class__ 선언  \n",
    "  \n",
    "2) *__Model Class__*  \n",
    "Layer 객체를 갖는 __Model Class__ 선언  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    '''\n",
    "        Fully Connected Layer Class\n",
    "    '''\n",
    "\n",
    "    def __init__(self, activation_type, layer_idx):\n",
    "        '''\n",
    "            Layer 객체의 생성자\n",
    "        '''\n",
    "        \n",
    "        # 이 Layer가 갖는 activation의 type\n",
    "        # C언어에서는 enum으로 지정한다.\n",
    "        self.activation = activation_type\n",
    "        \n",
    "        # 이 Layer가 몇 번째 Layer인지 나타내는 index 변수\n",
    "        # input layer를 포함한다.\n",
    "        self.layer_idx = layer_idx\n",
    "        \n",
    "        # weight\n",
    "        # 현재 Layer가 갖는 가중치값이다.\n",
    "        # Size : (prev_layer_node_num, current_layer_node_num)\n",
    "        self.weight = []\n",
    "        \n",
    "        # bias\n",
    "        # 현재 Layer가 갖는 편향값이다.\n",
    "        # Size : (current_layer_node_num, 1)\n",
    "        self.bias = []\n",
    "        \n",
    "        # backward pass 과정에 필요한 값을 저장하는 cache list\n",
    "        # weight와 bias는 Layer 자체에서 계속해서 저장하므로,\n",
    "        # prev_activation과 Z값만 저장하면 된다.\n",
    "        self.cache = {}\n",
    "        \n",
    "        self.dW = []\n",
    "        self.db = []\n",
    "\n",
    "    def initialize_parameter(self, cur_layer_node_num, prev_layer_node_num, layer_num):\n",
    "        '''\n",
    "            현재 Layer의 Parmeter(Weight, Bias)값을 초기화하는 메서드\n",
    "        '''\n",
    "        \n",
    "        # Layer를 구성하는 parameter를 initialize 한다.\n",
    "        \n",
    "        # 1. bias\n",
    "        # bias는 size가 (cur_layer_node_num, 1)이므로, 모두 0으로 초기화한다.\n",
    "        \n",
    "        # 2. weight\n",
    "        # weight는 size가 (cur_layer_node_num, prev_layer_node_num)이다.\n",
    "        \n",
    "        # output layer라면 activation으로 Softmax를 사용하므로, Xaiver Initialization을 사용한다.\n",
    "        if (self.layer_idx == layer_num-1):\n",
    "            self.weight, self.bias, self.dW, self.db = init_params_by_type(cur_layer_node_num, prev_layer_node_num, \"xavier\")\n",
    "        # hidden layer라면 activation으로 ReLU를 사용하므로, He Initialization을 사용한다.\n",
    "        else:\n",
    "            self.weight, self.bias, self.dW, self.db = init_params_by_type(cur_layer_node_num, prev_layer_node_num, \"he\")\n",
    "    \n",
    "    def save_cache_variable(self, prev_activation, Z):\n",
    "        '''\n",
    "            forward pass에서, 각 Layer의 학습에 필요한\n",
    "            cache 값을 저장하는 메서드\n",
    "        '''\n",
    "        \n",
    "        self.cache['prev_activation'] = prev_activation\n",
    "        self.cache['Z'] = Z\n",
    "        \n",
    "    def save_gradient(self, dW, db, cur_layer_node_num, prev_layer_node_num):\n",
    "        '''\n",
    "            backward pass에서, 한 개의 example에 대해서 구해진 gradient를\n",
    "            현재 Layer의 dW, db에 더해주는 메서드. \n",
    "\n",
    "            parameter update는 전체 example에 대한 총합을 구하는 과정이며, \n",
    "            각 example에 대한 값으 모두 구한 뒤에 update_parameter() 메서드를 통해 \n",
    "            example 개수로 나눠서 Batch Gradient Descent를 적용한다.\n",
    "        '''\n",
    "        \n",
    "        for i in range(cur_layer_node_num):\n",
    "            self.db[i] += db[i]\n",
    "            for j in range(prev_layer_node_num):\n",
    "                self.dW[i][j] += dW[i][j]\n",
    "    \n",
    "    def update_parameter(self, train_data_num, cur_layer_node_num, prev_layer_node_num, learning_rate):\n",
    "        '''\n",
    "            구해진 전체 dW, db를 exaple의 개수로 나눠서 1 epoch에 대한 gradient를 구하는 메서드\n",
    "        '''\n",
    "        \n",
    "        for i in range(cur_layer_node_num):\n",
    "            self.db[i] /= train_data_num\n",
    "            for j in range(prev_layer_node_num):\n",
    "                self.dW[i][j] /= train_data_num\n",
    "        \n",
    "        for i in range(cur_layer_node_num):\n",
    "            self.bias[i] -= learning_rate * self.db[i]\n",
    "            for j in range(prev_layer_node_num):\n",
    "                self.weight[i][j] -= learning_rate * self.dW[i][j]\n",
    "        \n",
    "    def clear_gradient(self, cur_layer_node_num, prev_layer_node_num):\n",
    "        '''\n",
    "            1 epoch에 대해서 학습을 진행한 뒤, dW, db 값을 0으로 초기화하는 메서드\n",
    "            이를 통해 다음 epoch에서의 gradient를 구해서 update를 진행할 수 있도록 한다.\n",
    "        '''\n",
    "        \n",
    "        for i in range(cur_layer_node_num):\n",
    "            self.db[i] = 0\n",
    "            for j in range(prev_layer_node_num):\n",
    "                self.dW[i][j] = 0\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *__Layer Class__*\n",
    "\n",
    "<br>\n",
    "\n",
    "1) __init()__  \n",
    "  \n",
    "    Layer Class의 생성자입니다.  \n",
    "    Layer는 다음과 같은 변수를 가집니다.  \n",
    "\n",
    "    \n",
    "     \n",
    "    * activation : 해당 Layer의 Activation Function을 결정합니다. (ReLU, Softmax)  \n",
    "\n",
    "    * layer_idx : 해당 Layer의 Index를 결정합니다. (Input Layer는 0)  \n",
    "\n",
    "    * weight : 해당 Layer의 Weight를 갖는 리스트입니다.  \n",
    "\n",
    "    * bias : 해당 Layer의 bias를 갖는 리스트입니다.  \n",
    "\n",
    "    * cache : Backward Pass에 사용할 Z값(레이어 별 행렬곱의 결과), previous Activation(현재 Layer의 Input)을 보관하는 객체입니다.  \n",
    "\n",
    "    * dW : Weight의 gradient를 보관하는 리스트입니다.  \n",
    "\n",
    "    * db : bias의 gradient를 보관하는 리스트입니다.  \n",
    "\n",
    "<br>\n",
    "\n",
    "2) __initialize_parameter()__  \n",
    "\n",
    "    Parameter를 초기화하는 함수입니다. 인자로 현재 레이어, 이전 레이어의 노드 개수를 전달받고,  \n",
    "    전체 레이어의 개수를 전달받습니다.  \n",
    "    \n",
    "    hidden layer는 ReLU를 사용하고, output layer는 Softmax를 사용합니다.  \n",
    "    각각의 type에 따라서 init_params_by_type() 함수를 사용해 Weight/bias를 초기화합니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "3) __save_cache_variable()__  \n",
    "\n",
    "    Backward Pass에서 Gradient를 만드는데 필요한 값을 Forward Pass 과정에서 저장하는 함수입니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "4) __save_gradient()__  \n",
    "\n",
    "    Backward Pass 과정에서 구한 gradient를 dW와 db에 저장하는 함수입니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "5) __update_parameter()__  \n",
    "\n",
    "    Backward Pass에서 구한 gradient를 이용해 parameter를 update하는 함수입니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "6) __clear_gradient()__  \n",
    "\n",
    "    Backward Pass에서 구한 gradient를 삭제하고, dW, db를 모두 0으로 초기화하는 함수입니다.  \n",
    "    한 번의 Backward Pass가 실행이 완료되면, 다음 Example에 대한 gradient를 구해야 하기 때문에 실행합니다.\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Model:\n",
    "    '''\n",
    "        FC Layer를 property로 갖는 Model Class\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, layer_num, epoch_num, learning_rate):\n",
    "        '''\n",
    "            Model 객체의 생성자\n",
    "        '''\n",
    "        \n",
    "        # input layer를 포함한 전체 layer의 개수\n",
    "        self.layer_num = layer_num\n",
    "        \n",
    "        # input layer를 포함하여, Layer 객체를 보관하는 리스트\n",
    "        self.layers = [] \n",
    "        \n",
    "        # 각 layer 별 node의 개수를 보관하는 리스트\n",
    "        # 추론/학습 과정에서 input layer의 노드 개수가 필요하므로, 노드 개수도 보관한다.\n",
    "        self.layer_node_num = []\n",
    "        \n",
    "        # train/test data\n",
    "        # 16*16 이미지가 하나의 example이므로, 하나의 example의 size는 (256, 1)\n",
    "        # 전체 데이터 셋이 m개일 때, 전체 data set의 size는 (256, m)이라고 할 수 있다.\n",
    "        self.train_data_num = 0 # 420개 (t, u, v, w, x, y, z 60세트)\n",
    "        self.train_data = [] # 420개의 train data를 보관하는 list\n",
    "        \n",
    "        self.test_data_num = 0 # 140개 (t, u, v, w, x, y, z 20세트)\n",
    "        self.test_data = [] # 140개의 test data를 보관하는 list\n",
    "        \n",
    "        # data target\n",
    "        # train/test에 대한 target이다.\n",
    "        # 하나의 target은 (1, 7) size의 list이며,\n",
    "        # train은 420개이므로, size는 (420, 7)\n",
    "        # test는 140개이므로, size는 (140, 7)\n",
    "        self.train_target = []\n",
    "        self.test_target = []\n",
    "        \n",
    "        # 전체 데이터셋을 학습할 횟수, epoch num\n",
    "        self.epoch_num = epoch_num\n",
    "        \n",
    "        # 학습률(learning rate)\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def load_data(self, train_data_path, test_data_path):\n",
    "        '''\n",
    "            지정된 경로에서 train/test data set를 불러오는 메서드\n",
    "        '''\n",
    "        \n",
    "        # train/test data를 정해진 경로에서부터 load하여 저장\n",
    "        self.train_data, self.train_target = parse_img_pixels(train_data_path, \"train\")\n",
    "        self.test_data, self.test_target = parse_img_pixels(test_data_path, \"test\")\n",
    "        \n",
    "        # train/test 데이터의 개수를 저장\n",
    "        self.train_data_num = len(self.train_data)\n",
    "        self.test_data_num = len(self.test_data)\n",
    "    \n",
    "    \n",
    "    def insert_new_layer(self, activation_type, layer_idx):\n",
    "        '''\n",
    "            Model에 새로운 Layer를 추가하는 메서드\n",
    "            \n",
    "            insert_new_layer는 반드시 input layer가 만들어진 이후에 실행되므로,\n",
    "            hidden layer부터 사용되기 때문에 layer_num의 indexing에는 문제 없음.\n",
    "        '''\n",
    "        \n",
    "        # layer_idx는 input layer를 포함했을 때, 해당 layer의 index이다.\n",
    "        # 예를 들어, 첫 번째 hidden layer의 index는 1이다. (input layer는 0)\n",
    "        layer = Layer(activation_type, layer_idx) # Layer 객체 생성\n",
    "        \n",
    "        if (activation_type != 'none'): # input layer에 대해서는 실행하지 않음.\n",
    "            cur_layer_idx = layer_idx\n",
    "            prev_layer_idx = cur_layer_idx-1\n",
    "            \n",
    "            # 현재 Layer가 갖는 Weight/bias를 초기화한다.\n",
    "            layer.initialize_parameter(cur_layer_node_num=self.layer_node_num[cur_layer_idx], \n",
    "                                    prev_layer_node_num=self.layer_node_num[prev_layer_idx],\n",
    "                                    layer_num=self.layer_num)\n",
    "        \n",
    "        # Layer 객체를 self.layers에 추가한다.\n",
    "        self.layers.append(layer)\n",
    "            \n",
    "    def forward_pass(self, data_index, data_type):\n",
    "        '''\n",
    "            추론(inference) 과정을 진행하는 메서드\n",
    "            hidden layer에서는 ReLU를 거쳐 activation을 내보내고,\n",
    "            output layer에서는 Softmax를 거쳐 t, u, v, w, x, y, z에 대한 확률값을 반환한다.\n",
    "        '''\n",
    "        \n",
    "        # activation이라는 변수는 다음 layer에 들어가는 input을 의미하며,\n",
    "        # for loop을 돌면서, 한 layer에서 반환된 activation을 다음 layer로 넘겨주는 역할이다.\n",
    "        # 구현 상의 편의를 위해 hidden layer 1에 들어가는 input data를 activation에 초기화한다.\n",
    "        # 즉, 이전 layer의 activation이 다음 layer에 대해서는 input이 되며, 이를 하나의 변수로 처리하는 것이다.\n",
    "        activation = 0\n",
    "        \n",
    "        # train/test에 따라서 처음에 선택되는 input값을 선택한다.\n",
    "        if (data_type == \"train\"):\n",
    "            activation = self.train_data[data_index]\n",
    "        elif (data_type == \"test\"):\n",
    "            activation = self.test_data[data_index]\n",
    "        \n",
    "        # forward pass, inference를 진행한다.\n",
    "        # 예를 들어, layer_num = 4일 경우 (input, hidden, hidden, output) 세 번의 loop가 동작하게 될 것이다.\n",
    "        for layer_index in range(1, self.layer_num):\n",
    "            cur_layer_idx = layer_index\n",
    "            prev_layer_idx = cur_layer_idx-1\n",
    "            \n",
    "            current_layer_node_num = self.layer_node_num[cur_layer_idx] # 현재 layer의 node 개수\n",
    "            prev_layer_node_num = self.layer_node_num[prev_layer_idx] # 이전 layer의 node 개수\n",
    "            \n",
    "            # prev_activation은 이전 layer에서 현재 layer로 전달되는 activation 값이다.\n",
    "            # 초기에는 input data가 activation임에 유의한다.\n",
    "            prev_activation = activation\n",
    "            \n",
    "            # 행렬곱을 진행하는 부분이다. \n",
    "            # 데이터에 weight를 곱하고 bias를 더한다.\n",
    "            # 그리고 이 연산의 결과를 Z라고 한다.\n",
    "            Z = dot_product(\n",
    "                prev_node_num=prev_layer_node_num, \n",
    "                cur_node_num=current_layer_node_num, \n",
    "                weight=self.layers[cur_layer_idx].weight, \n",
    "                data=activation\n",
    "            )\n",
    "            \n",
    "            # 출력된 z vector에 대해서 bias를 각각 더해준다.\n",
    "            for i in range(self.layer_node_num[cur_layer_idx]):\n",
    "                Z[i] += self.layers[cur_layer_idx].bias[i]\n",
    "                \n",
    "            # backward_pass에 필요한 값들을 cache에 저장해둔다.\n",
    "            # weight와 bias는 layer 객체에 저장되어 있으므로, prev_activation과 z만 보관한다.\n",
    "            self.layers[cur_layer_idx].save_cache_variable(prev_activation, Z)\n",
    "            \n",
    "            # z를 활성화 함수에 넣어서 activation 값을 얻는다.\n",
    "            if (self.layers[cur_layer_idx].activation == \"relu\"):\n",
    "                activation = element_wise_relu(Z)\n",
    "            elif (self.layers[cur_layer_idx].activation == \"softmax\"):\n",
    "                activation = softmax(Z)\n",
    "        \n",
    "        # forward pass의 최종 출력(output)을 return한다.\n",
    "        return activation\n",
    "\n",
    "    def calc_output_layer_dZ(self, output_layer_node_num, output, cur_target):\n",
    "        '''\n",
    "            Softmax(Output Layer Activation Function) + Cross Entropy(Loss Function)를 사용하면\n",
    "            미분 식이 매우 간단해져서 gradient를 쉽게 구할 수 있다.\n",
    "            이 메서드에서는 아래 for loop 계산을 통해서, output layer에 대한 dZ를 구해 return 한다.\n",
    "        '''\n",
    "        dZ = []\n",
    "        \n",
    "        for idx in range(output_layer_node_num):\n",
    "            dZ.append(output[idx] - cur_target[idx])\n",
    "            \n",
    "        return dZ\n",
    "    \n",
    "    def calc_hidden_layer_dZ(self, dA, cur_layer_idx, cur_layer_node_num):\n",
    "        '''\n",
    "            hidden layer에서의 dZ를 계산하는 함수\n",
    "            ReLU 공식에 따라, Forward 과정에서 Z값이 0보다 작거나 같았다면\n",
    "            Gradient는 0이고, Z값이 0보다 크다면 Gradient는 1이다.\n",
    "            이에 따라 Chain Rule을 사용하여 모든 Layer에 대한 dZ Gradient를 계산한다.\n",
    "        '''\n",
    "\n",
    "        # forward pass에서 계산했던 Z값을 cache로부터 가져온다.\n",
    "        Z = self.layers[cur_layer_idx].cache['Z']\n",
    "        \n",
    "        dZ_value = [] # dA/dZ를 저장하는 리스트\n",
    "        \n",
    "        # ReLU의 Gradient는 Z값이 0보다 클 경우 1, 0보다 작을 경우 0으로 처리하므로\n",
    "        # 다음과 같이 Z를 탐색하며 0보다 클 경우 dZ_value에 1을 추가, 작다면 0을 추가한다.\n",
    "        for i in range(cur_layer_node_num):\n",
    "            if (Z[i] < 0):\n",
    "                dZ_value.append(0)\n",
    "            else:\n",
    "                dZ_value.append(1)\n",
    "    \n",
    "        dZ = []\n",
    "        \n",
    "        # dZ를 계산한다. dZ는 dA*dZ_value이다.\n",
    "        # dA : 앞의 Layer에서 넘어온 미분값\n",
    "        # dZ_value : 위에서 구한, ReLU에 대한 미분값\n",
    "        # 둘을 곱하면 완전한 dZ를 얻게 된다.\n",
    "        for i in range(cur_layer_node_num):\n",
    "            dZ.append(dA[i]*dZ_value[i])\n",
    "        \n",
    "        return dZ\n",
    "\n",
    "    def calc_dW(self, cur_layer_node_num, prev_layer_node_num, cur_layer_idx, dZ):\n",
    "        '''\n",
    "            Output Layer의 Weight Gradient를 계산하는 함수\n",
    "        '''\n",
    "        \n",
    "        # forward pass 과정에서 저장해 두었던 prev_activation을 cache에서 불러온다.\n",
    "        prev_activation = self.layers[cur_layer_idx].cache['prev_activation']\n",
    "        dW = []\n",
    "        \n",
    "        # Output Layer의 Weight Gradient 계산\n",
    "        for i in range(cur_layer_node_num): \n",
    "            weight_row = []\n",
    "            for j in range(prev_layer_node_num):\n",
    "               weight_value = dZ[i] * prev_activation[j]\n",
    "               weight_row.append(weight_value)\n",
    "            dW.append(weight_row)\n",
    "        \n",
    "        return dW\n",
    "    \n",
    "    def calc_db(self, dZ):\n",
    "        '''\n",
    "            db를 계산하는 메서드\n",
    "            현재 사용하는 모델에서 db의 gradient는 dZ와 동일하므로,\n",
    "            dZ를 그대로 return해준다. => b로 편미분하면 db = dZ이다.\n",
    "            (추후에 변경될 가능성을 고려하여 모듈화함)\n",
    "        '''\n",
    "        \n",
    "        return dZ\n",
    "        \n",
    "    def calc_dA_prev(self, cur_layer_node_num, prev_layer_node_num, cur_layer_idx, dZ):\n",
    "        '''\n",
    "            현재 레이어의 prev_activation에 대한 gradient인 dA_prev를 계산하는 메서드\n",
    "            이 값을 계산하여 이전 레이어의 gradient를 계산하는데 전달해 주어야 한다.\n",
    "            (Hidden Layer는 직접적인 Error를 구할 수 없으며, Output Layer에서 Gradient를 계산하며\n",
    "            점차 이전 노드로 전파해 주는 방식을 사용해야 한다.)\n",
    "        '''\n",
    "\n",
    "        dA_prev = []\n",
    "        \n",
    "        for i in range(prev_layer_node_num):\n",
    "            dA_prev_value = 0\n",
    "            \n",
    "            for j in range(cur_layer_node_num):\n",
    "                dA_prev_value += self.layers[cur_layer_idx].weight[j][i] * dZ[j]\n",
    "            dA_prev.append(dA_prev_value)\n",
    "        \n",
    "        return dA_prev\n",
    "\n",
    "    def calc_softmax_gradient(self, cur_layer_idx, cur_layer_node_num, output, train_data_index):\n",
    "        '''\n",
    "            Output Layer의 Gradient를 계산하는 메서드\n",
    "            dW, db를 계산하기 위해서는 dA_prev, dZ가 필요하다.\n",
    "            \n",
    "            그런데 현재 Output Layer가 Softmax를 사용하기 때문에\n",
    "            dZ를 간단하게 축약하여 구할 수 있으며(calc_output_layer_dZ), \n",
    "            이 값과 weight, bias의 편미분을 곱해서 계산하면 gradient를 구할 수 있다.\n",
    "            (calc_dW, calc_db)\n",
    "            \n",
    "            또한 Output Layer에서는 dA_prev를 계산하여 이전 레이어로 전파해 주어야 하기 때문에\n",
    "            calc_dA_prev를 실행하여 값을 구하고 return 한다.\n",
    "        '''\n",
    "\n",
    "        # Gradient 계산 과정에서 Output 값이 필요하므로 target 값을 가져온다.\n",
    "        # 현재 학습을 진행 중인 example의 index는 train_data_index이다.\n",
    "        cur_target = self.train_target[train_data_index] \n",
    "\n",
    "        # 행렬곱 연산을 수행하기 위해서 이전 layer의 index와 node_num을 가져온다.\n",
    "        prev_layer_idx = cur_layer_idx-1\n",
    "        prev_layer_node_num = self.layer_node_num[prev_layer_idx]\n",
    "    \n",
    "        # dW와 db를 계산한다.\n",
    "        # 1) dW, db를 계산하기 위해서는 dZ가 필요하므로, 먼저 dZ를 계산한다.\n",
    "        # 2) dZ와 prev_activation을 바탕으로 dW를 계산한다.\n",
    "        # 3) dZ를 바탕으로 db를 계산한다.\n",
    "        dZ = self.calc_output_layer_dZ(cur_layer_node_num, output, cur_target)\n",
    "        dW = self.calc_dW(cur_layer_node_num, prev_layer_node_num, cur_layer_idx, dZ)\n",
    "        db = self.calc_db(dZ)\n",
    "        \n",
    "        # dZ와 Weight를 이용해, 이전 layer로 전달해 줄 dA_prev(prev_activation의 gradient)를 계산한다.\n",
    "        dA_prev = self.calc_dA_prev(cur_layer_node_num, prev_layer_node_num, cur_layer_idx, dZ)\n",
    "        \n",
    "        # 구한 dW, db, dA_prev를 반환한다.\n",
    "        return dW, db, dA_prev\n",
    "       \n",
    "    # 인자로 넘어오는 layer_idx는 input layer를 포함한 index임에 유의한다.\n",
    "    def calc_relu_gradient(self, dA, cur_layer_node_num, cur_layer_idx):\n",
    "        '''\n",
    "            Hidden Layer의 Gradient를 계산하는 메서드\n",
    "            \n",
    "            dA : 현재 노드의 Activation에 대한 Gradient이다. 이 값은\n",
    "            next_layer에서 연산하여 현재 레이어로 전달해 주는 값이다.\n",
    "\n",
    "            layer_idx : 현재 Layer의 index이다.\n",
    "        '''\n",
    "        \n",
    "        # 행렬곱 연산을 수행하기 위해서 이전 layer의 index와 node_num을 가져온다.\n",
    "        prev_layer_idx = cur_layer_idx-1\n",
    "        prev_layer_node_num = self.layer_node_num[prev_layer_idx]\n",
    "        \n",
    "        # dW와 db를 계산한다.\n",
    "        # 1) dW, db를 계산하기 위해서는 dZ가 필요하므로, 먼저 dZ를 계산한다.\n",
    "        # 2) dZ와 prev_activation을 바탕으로 dW를 계산한다.\n",
    "        # 3) dZ를 바탕으로 db를 계산한다.\n",
    "        dZ = self.calc_hidden_layer_dZ(dA, cur_layer_idx, cur_layer_node_num)\n",
    "        dW = self.calc_dW(cur_layer_node_num, prev_layer_node_num, cur_layer_idx, dZ)\n",
    "        db = self.calc_db(dZ)\n",
    "        \n",
    "        # dZ와 Weight를 이용해, 이전 layer로 전달해 줄 dA_prev(prev_activation의 gradient)를 계산한다.\n",
    "        dA_prev = self.calc_dA_prev(cur_layer_node_num, prev_layer_node_num, cur_layer_idx, dZ)\n",
    "        \n",
    "         # 구한 dW, db, dA_prev를 반환한다.\n",
    "        return dW, db, dA_prev\n",
    "    \n",
    "    def backward_pass(self, output, train_data_index): \n",
    "        '''\n",
    "            학습 과정(Backward Pass)이자, parameter update 과정이다.\n",
    "            Gradient Descent에 의해 Error를 바탕으로 학습을 진행한다.\n",
    "        '''\n",
    "        \n",
    "        # Output Layer에 대한 gradient 계산 진행 (Softmax)\n",
    "        output_layer_idx = self.layer_num-1 # output_layer_idx == self.layer_num-1\n",
    "        output_layer_node_num = self.layer_node_num[output_layer_idx]\n",
    "        \n",
    "        prev_layer_idx = output_layer_idx-1 # the prev layer of output layer\n",
    "        prev_layer_node_num = self.layer_node_num[prev_layer_idx]\n",
    "        \n",
    "        # output layer에 대한 gradient를 계산한다. (Softmax)\n",
    "        dW, db, dA_prev = self.calc_softmax_gradient(output_layer_idx, output_layer_node_num, output, train_data_index)\n",
    "        \n",
    "        # 전달받은 dW와 db를 output layer에 저장한다.\n",
    "        self.layers[output_layer_idx].save_gradient(dW, db, output_layer_node_num, prev_layer_node_num)\n",
    "        \n",
    "        # hidden layer에 대한 gradient를 계산한다. (ReLU)\n",
    "        # layer_idx는 input layer를 포함한 index.\n",
    "        for layer_idx in range(self.layer_num-2, 0, -1):\n",
    "            cur_layer_idx = layer_idx\n",
    "            prev_layer_idx = cur_layer_idx-1\n",
    "            \n",
    "            cur_layer_node_num = self.layer_node_num[cur_layer_idx]\n",
    "            prev_layer_node_num = self.layer_node_num[prev_layer_idx]\n",
    "            \n",
    "            dW, db, dA_prev = self.calc_relu_gradient(dA_prev, cur_layer_node_num, cur_layer_idx)\n",
    "            \n",
    "            self.layers[cur_layer_idx].save_gradient(dW, db, cur_layer_node_num, prev_layer_node_num)\n",
    "            \n",
    "    def isCorrect(self, output, train_data_index):\n",
    "        '''\n",
    "            inference의 결과가 정답인지 확인하는 메서드\n",
    "        '''\n",
    "\n",
    "        # 1 epoch 내에서 1 example에 대한 결과의 정답 여부를 체크\n",
    "        predict_index = output.index(max(output))\n",
    "        target_index = self.train_target[train_data_index].index(1)\n",
    "        \n",
    "        # 정답일 경우 correct를 1 증가시킨다.\n",
    "        if predict_index == target_index:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def train(self):\n",
    "        '''\n",
    "            training 과정을 진행하는 메서드\n",
    "            training data에 대한 inference/learning을 수행한다.\n",
    "        '''\n",
    "\n",
    "        # 정해진 epoch num만큼 학습을 진행\n",
    "        total = self.train_data_num # train data set의 개수\n",
    "        \n",
    "        for epoch in range(self.epoch_num):\n",
    "            # 1 epoch 학습을 진행\n",
    "            print(f\"Epoch {epoch}\")\n",
    "        \n",
    "            correct = 0\n",
    "            for train_data_index in range(self.train_data_num):\n",
    "                # 1개의 example에 대한 forward_pass 결과를 얻는다.\n",
    "                output = self.forward_pass(train_data_index, \"train\")\n",
    "                \n",
    "                # 정답일 경우 correct를 1 증가시킨다.\n",
    "                if self.isCorrect(output, train_data_index):\n",
    "                    correct += 1\n",
    "                \n",
    "                # Loss를 backward_pass에 넣어서 1 example에 대한 gradient를 계산한다.\n",
    "                self.backward_pass(output, train_data_index)\n",
    "                \n",
    "            # 1 epoch에 대한 Accuracy를 출력\n",
    "            print (f\"Accuracy : {correct/total}\")            \n",
    "            \n",
    "            # 1 epoch에 대한 가중치 업데이트 진행(Batch Gradient Descent)\n",
    "            for layer_idx in range(1, self.layer_num):\n",
    "                cur_layer_idx = layer_idx\n",
    "                prev_layer_idx = cur_layer_idx-1\n",
    "                \n",
    "                cur_layer_node_num = self.layer_node_num[cur_layer_idx]\n",
    "                prev_layer_node_num = self.layer_node_num[prev_layer_idx]\n",
    "                \n",
    "                self.layers[layer_idx].update_parameter(self.train_data_num, cur_layer_node_num, \n",
    "                                                        prev_layer_node_num, self.learning_rate)    \n",
    "                \n",
    "                self.layers[layer_idx].clear_gradient(cur_layer_node_num, prev_layer_node_num) \n",
    "    \n",
    "    def predict(self):\n",
    "        '''\n",
    "            학습된 Parameter 값을 가지고 test set에 대한 forward pass를 진행한다.\n",
    "        '''\n",
    "        \n",
    "        correct = 0\n",
    "        wrong = 0\n",
    "        \n",
    "        for test_data_idx in range(self.test_data_num):\n",
    "            output = self.forward_pass(test_data_idx, \"test\")\n",
    "            predict_index = output.index(max(output))\n",
    "            target_index = self.test_target[test_data_idx].index(1)\n",
    "            \n",
    "            if predict_index == target_index:\n",
    "                correct += 1\n",
    "            else:\n",
    "                wrong += 1\n",
    "        print(f\"Correct : {correct}\")\n",
    "        print(f\"Total : {self.test_data_num}\")\n",
    "        print(f\"정답률 : {correct/self.test_data_num}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *__Model Class__*\n",
    "\n",
    "<br>\n",
    "\n",
    "1) __init()__  \n",
    "\n",
    "    Layer Class의 생성자입니다.  \n",
    "    Layer는 다음과 같은 Property를 가집니다.  \n",
    "\n",
    "    \n",
    "     \n",
    "    * layer_num : Model을 구성하는 전체 Layer의 개수입니다. Input Layer를 포함합니다.\n",
    "\n",
    "    * layers : Model을 구성하는 Layer의 객체를 보관하는 리스트입니다.\n",
    "\n",
    "    * layer_node_num : Layer 별 Node의 개수를 보관하는 리스트입니다.\n",
    "\n",
    "    * train_data_num : Training Data의 전체 개수입니다.\n",
    "\n",
    "    * train_data : Training Data를 보관하는 리스트입니다.\n",
    "\n",
    "    * test_data_num : Test Data의 전체 개수입니다.\n",
    "\n",
    "    * test_data : Test Data를 보관하는 리스트입니다.\n",
    "\n",
    "    * train_target : Training Data에 대한 Target Label Vector를 보관하는 리스트입니다.\n",
    "\n",
    "    * test_target : Test Data에 대한 Target Label Vector를 보관하는 리스트입니다.\n",
    "\n",
    "    * epoch_num : 전체 Epoch 횟수입니다.\n",
    "\n",
    "    * learning_rate : 학습 과정에서 적용할 학습률입니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "2) __load_data()__\n",
    "\n",
    "    Training/Test Data를 load하는 함수입니다.  \n",
    "    parse_img_pixels() 함수를 이용하여 각각의 path에서 데이터를 load하고,  \n",
    "    data/target을 구분하여 반환받아 클래스에 저장합니다.  \n",
    "    그리고, 각 Training/Test의 개수를 data_num에 저장합니다.\n",
    "  \n",
    "<br>\n",
    "\n",
    "3) __insert_new_layer()__\n",
    "\n",
    "    Model을 구성하는 새로운 Layer를 만들고, Layer의 객체를 self.layers에 저장하는 함수입니다.  \n",
    "    인자로 activation type을 전달받아 Layer에 전달해주며, Input Layer는 layer_node_num과  \n",
    "    layers의 index가 매칭되도록 만들기 위한 용도이므로, Layer 객체를 만들어서  \n",
    "    self.layers에 추가하지만, 가중치를 초기화 하지는 않습니다.  \n",
    "\n",
    "<br>\n",
    "\n",
    "4) __forward_pass()__ \n",
    "  \n",
    "    Forward Pass를 진행하는 함수입니다.  \n",
    "    Train/Test 과정에서 공통적으로 사용하기 위해 data_type(train or test)를 전달받아 구분합니다.  \n",
    "    처음에 input 데이터를 activation 변수에 넣고, 각 Layer를 거치면서 생성되는  \n",
    "    Layer별 새로운 activation값으로 계속해서 갱신합니다.  \n",
    "    Input Layer는 가중치를 가지지 않으므로, hidden Layer부터 계산을 하도록  \n",
    "    for loop를 돌리며, 안에서는 다음 두 개의 연산을 진행합니다.  \n",
    "      \n",
    "    1) Z = WX + b (Input X에 가중치 Matrix W를 곱하고, bias b를 더해준다.)  \n",
    "    2) a = Activation(Z) (구해진 Z값을 activation function에 넣어 activation을 얻는다).  \n",
    "      \n",
    "    이렇게 최종적으로 얻어낸 output(activation 변수에 저장되어 있음)을 return합니다.\n",
    "    \n",
    "<br>\n",
    "\n",
    "5) __calc_output_layer_dZ()__  \n",
    "\n",
    "    Backward Pass 과정에서 output layer(Softmax Layer)에서의 dZ를 계산하는 함수입니다.  \n",
    "    본 프로젝트에서 dA, dZ의 의미는, 다음과 같습니다.  \n",
    "    * dA : Loss를 해당 layer의 activation으로 편미분한 값.  \n",
    "    * dZ : Loss를 해당 layer의 Z로 편미분한 값.  \n",
    "      \n",
    "    Backward Pass에서 parameter의 gradient를 구하기 위해 dZ가 필요하므로, 이 함수에서 계산합니다.  \n",
    "    그런데, output layer는 Softmax를 Activation Function으로 사용합니다.  \n",
    "    현재 Loss Function을 Cross Entropy로 사용하고 있으므로, 식을 미분해서 구해보면  \n",
    "    output layer의 dZ는 *__output - target__* 으로 간단하게 축약됩니다.  \n",
    "    따라서 forward pass에서 전달받은 output과 target을 사용해 dZ를 구하고 반환합니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "6) __calc_hidden_layer_dZ()__  \n",
    "  \n",
    "    Backward Pass 과정에서 hidden layer(ReLU)에서의 dZ를 계산하는 함수입니다.  \n",
    "    ReLU를 사용한 dZ는, ReLU의 정의에 따라 Z값이 0보다 작거나 같으면 0, 0보다 크면 1입니다.  \n",
    "    이 방식을 사용해 dZ를 구하고 반환합니다.  \n",
    "\n",
    "<br>\n",
    "\n",
    "7) __calc_dW()__  \n",
    "  \n",
    "    Backward Pass 과정에서 현재 Layer의 dW(Loss를 W로 편미분한 것)를 계산하는 함수입니다.  \n",
    "    dW를 Chain Rule로 미분해보면, 계산하는데 Forward Pass에서  \n",
    "    저장한, 해당 Layer의 Input(이전 Layer의 Activation)이 필요하게 됩니다.  \n",
    "    따라서 이 값을 cache에서부터 불러오고, 이 값을 활용해 dW를 계산합니다.  \n",
    "      \n",
    "    실제 수학적 수식으로 구현하기 위해서는 Transpose를 적절하게 사용해야 하지만,  \n",
    "    List를 이용해 Transpose를 구현하는 것은 메모리 관점에서 비효율적이라고 생각했습니다.  \n",
    "\n",
    "    (예컨대, list[10][1]에서 10개의 값을 탐색하는 것은,  \n",
    "    list[1][10]에서 10개의 값을 탐색하는 것이 더 비효율적이다.)  \n",
    "     \n",
    "    따라서 for loop에서 적절하게 식을 설정하여 동일한 연산이 수행되도록 만들었습니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "8) __calc_db()__  \n",
    "\n",
    "    Backward Pass 과정에서 현재 Layer의 db를 계산하는 함수입니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "9) __calc_dA_prev()__  \n",
    "\n",
    "    Backward Pass 과정에서, 현재 Layer로부터, 이전 Layer의 Activation(Prev Activation)에 대한  \n",
    "    gradient를 구하는 함수입니다. 이것을 구하고 이전 Layer로 넘겨주어야만  \n",
    "    이전 Layer의 gradient를 구할 수 있기 때문에 사용합니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "10) __calc_softmax_gradient()__  \n",
    "\n",
    "    Softmax를 사용하는 Layer에서의 gradient를 구하는 함수입니다.  \n",
    "    dW, db를 구하기 위해서 calc_output_layer_dZ(), calc_dW(), calc_db()를 사용하고,  \n",
    "    calc_dA_prev()를 구해서 이전 Layer로 넘겨줍니다.  \n",
    "\n",
    "<br>\n",
    "\n",
    "11) __calc_relu_gradient()__  \n",
    "\n",
    "    ReLU를 사용하는 Layer에서의 gradient를 구하는 함수입니다.  \n",
    "    dW, db를 구하기 위해서 calc_hidden_layer_dZ(), calc_dW(), calc_db()를 사용하고,  \n",
    "    calc_dA_prev()를 구해서 이전 Layer로 넘겨줍니다.  \n",
    "\n",
    "<br>\n",
    "\n",
    "12) __backward_pass()__  \n",
    "\n",
    "    Backward Pass의 전체 과정을 진행하는 함수입니다.  \n",
    "    Backward Pass는 하나의 Example에 대해서 진행하며, 전체 Layer에서의  dW, db를 구하고,  \n",
    "    이 값을 저장하는 역할을 합니다. 이후에 parameter update 함수들을 이용하여  \n",
    "    실제로 parameter를 update합니다.  \n",
    "\n",
    "<br>\n",
    "\n",
    "13) __isCorrect()__  \n",
    "\n",
    "    Forward Pass에서 최종적으로 나온 Output이 정답과 일치하는지 검사하는 함수입니다.  \n",
    "    정답과 일치한다면 True를, 틀렸다면 False를 return하여 현재 epoch에서의 Accuracy를  \n",
    "    구하는데 사용합니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "14) __train()__  \n",
    "\n",
    "    Training Data에 대해서 Forward Pass와 Backward Pass,  \n",
    "    그리고 Parameter Update를 수행하는 함수입니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "15) __predict()__  \n",
    "\n",
    "    Test Data에 대해서 Forward Pass를 진행하고, 학습 결과를 Accuracy로 확인하는 함수입니다.\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"./train/\"\n",
    "test_data_path = \"./test/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 설정\n",
    "\n",
    "모델의 아키텍처를 설정하는 부분입니다.  \n",
    "\n",
    "    * layer_num = 4\n",
    "    * epoch_num = 200\n",
    "    * learning_rate = 0.03\n",
    "\n",
    "    * input_node_num = 256 (16*16 이미지의 flatten)\n",
    "    * output_node_num = 7 (ex. [0 1 0 0 0 0 0])\n",
    "    * hidden_node_num = [96, 48] (hidden layer를 구성하는 node 개수)\n",
    "    \n",
    "  \n",
    "이 부분을 수정하여 Model의 구조를 변경하며 Train/Test를 진행할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input layer를 포함한 전체 layer의 수\n",
    "layer_num = 4\n",
    "\n",
    "# epoch 횟수\n",
    "epoch_num = 200\n",
    "\n",
    "# learning_rate, 학습률\n",
    "learning_rate = 0.03\n",
    "\n",
    "input_node_num = 256 # input layer의 node num\n",
    "output_node_num = 7 # output layer의 node num\n",
    "hidden_node_num = [96, 48] # hidden layer들의 node num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* random.seed(42)\n",
    "\n",
    "    random 함수에 대한 seed 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 객체 생성\n",
    "model = Model(\n",
    "    layer_num=layer_num, \n",
    "    epoch_num=epoch_num, \n",
    "    learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 모델 객체를 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model에 layer_num만큼 Layer를 추가한다.\n",
    "# 이 때, input layer는 실제 Layer 객체로 구성하지는 않고\n",
    "# 계산을 편리하게 하기 위해 self.layer_num list에 input_node_num만 추가한다.\n",
    "\n",
    "model.layer_node_num.clear()\n",
    "model.layers.clear()\n",
    "\n",
    "for layer_idx in range(model.layer_num):\n",
    "    # input layer\n",
    "    if (layer_idx == 0):\n",
    "        model.layer_node_num.append(input_node_num)\n",
    "        model.insert_new_layer(activation_type='none', layer_idx=layer_idx)\n",
    "        \n",
    "    # output layer(activation은 softmax)\n",
    "    elif (layer_idx == layer_num-1):\n",
    "        model.layer_node_num.append(output_node_num)\n",
    "        model.insert_new_layer(activation_type='softmax', layer_idx=layer_idx)\n",
    "\n",
    "    # hidden layer(activation은 relu)\n",
    "    else:\n",
    "        model.layer_node_num.append(hidden_node_num[layer_idx-1]) # hidden node의 개수 list를 탐색하며 추가\n",
    "        model.insert_new_layer(activation_type='relu', layer_idx=layer_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 모델에 Layer를 추가한다. 설정한 모델 아키텍처를 바탕으로, insert_new_layer() 메서드를 실행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델에 데이터 셋 로드\n",
    "model.load_data(train_data_path, test_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Data Set(Train_Data, Train_Label/Test_Data, Test_Label)를 Load한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.14285714285714285\n",
      "Epoch 1\n",
      "Accuracy : 0.11666666666666667\n",
      "Epoch 2\n",
      "Accuracy : 0.14047619047619048\n",
      "Epoch 3\n",
      "Accuracy : 0.16904761904761906\n",
      "Epoch 4\n",
      "Accuracy : 0.1976190476190476\n",
      "Epoch 5\n",
      "Accuracy : 0.2357142857142857\n",
      "Epoch 6\n",
      "Accuracy : 0.2761904761904762\n",
      "Epoch 7\n",
      "Accuracy : 0.3238095238095238\n",
      "Epoch 8\n",
      "Accuracy : 0.38095238095238093\n",
      "Epoch 9\n",
      "Accuracy : 0.45\n",
      "Epoch 10\n",
      "Accuracy : 0.4976190476190476\n",
      "Epoch 11\n",
      "Accuracy : 0.5428571428571428\n",
      "Epoch 12\n",
      "Accuracy : 0.6047619047619047\n",
      "Epoch 13\n",
      "Accuracy : 0.6476190476190476\n",
      "Epoch 14\n",
      "Accuracy : 0.6880952380952381\n",
      "Epoch 15\n",
      "Accuracy : 0.7119047619047619\n",
      "Epoch 16\n",
      "Accuracy : 0.7285714285714285\n",
      "Epoch 17\n",
      "Accuracy : 0.7357142857142858\n",
      "Epoch 18\n",
      "Accuracy : 0.7547619047619047\n",
      "Epoch 19\n",
      "Accuracy : 0.7619047619047619\n",
      "Epoch 20\n",
      "Accuracy : 0.7785714285714286\n",
      "Epoch 21\n",
      "Accuracy : 0.7928571428571428\n",
      "Epoch 22\n",
      "Accuracy : 0.8023809523809524\n",
      "Epoch 23\n",
      "Accuracy : 0.8166666666666667\n",
      "Epoch 24\n",
      "Accuracy : 0.8238095238095238\n",
      "Epoch 25\n",
      "Accuracy : 0.830952380952381\n",
      "Epoch 26\n",
      "Accuracy : 0.85\n",
      "Epoch 27\n",
      "Accuracy : 0.85\n",
      "Epoch 28\n",
      "Accuracy : 0.8547619047619047\n",
      "Epoch 29\n",
      "Accuracy : 0.8571428571428571\n",
      "Epoch 30\n",
      "Accuracy : 0.8595238095238096\n",
      "Epoch 31\n",
      "Accuracy : 0.861904761904762\n",
      "Epoch 32\n",
      "Accuracy : 0.8738095238095238\n",
      "Epoch 33\n",
      "Accuracy : 0.8809523809523809\n",
      "Epoch 34\n",
      "Accuracy : 0.8833333333333333\n",
      "Epoch 35\n",
      "Accuracy : 0.888095238095238\n",
      "Epoch 36\n",
      "Accuracy : 0.8928571428571429\n",
      "Epoch 37\n",
      "Accuracy : 0.8952380952380953\n",
      "Epoch 38\n",
      "Accuracy : 0.8928571428571429\n",
      "Epoch 39\n",
      "Accuracy : 0.8976190476190476\n",
      "Epoch 40\n",
      "Accuracy : 0.9\n",
      "Epoch 41\n",
      "Accuracy : 0.9\n",
      "Epoch 42\n",
      "Accuracy : 0.9023809523809524\n",
      "Epoch 43\n",
      "Accuracy : 0.9023809523809524\n",
      "Epoch 44\n",
      "Accuracy : 0.9047619047619048\n",
      "Epoch 45\n",
      "Accuracy : 0.9095238095238095\n",
      "Epoch 46\n",
      "Accuracy : 0.9119047619047619\n",
      "Epoch 47\n",
      "Accuracy : 0.919047619047619\n",
      "Epoch 48\n",
      "Accuracy : 0.9214285714285714\n",
      "Epoch 49\n",
      "Accuracy : 0.9214285714285714\n",
      "Epoch 50\n",
      "Accuracy : 0.9214285714285714\n",
      "Epoch 51\n",
      "Accuracy : 0.9238095238095239\n",
      "Epoch 52\n",
      "Accuracy : 0.9261904761904762\n",
      "Epoch 53\n",
      "Accuracy : 0.9261904761904762\n",
      "Epoch 54\n",
      "Accuracy : 0.9285714285714286\n",
      "Epoch 55\n",
      "Accuracy : 0.9285714285714286\n",
      "Epoch 56\n",
      "Accuracy : 0.9285714285714286\n",
      "Epoch 57\n",
      "Accuracy : 0.9333333333333333\n",
      "Epoch 58\n",
      "Accuracy : 0.9333333333333333\n",
      "Epoch 59\n",
      "Accuracy : 0.9357142857142857\n",
      "Epoch 60\n",
      "Accuracy : 0.9357142857142857\n",
      "Epoch 61\n",
      "Accuracy : 0.9357142857142857\n",
      "Epoch 62\n",
      "Accuracy : 0.9380952380952381\n",
      "Epoch 63\n",
      "Accuracy : 0.9404761904761905\n",
      "Epoch 64\n",
      "Accuracy : 0.9404761904761905\n",
      "Epoch 65\n",
      "Accuracy : 0.9380952380952381\n",
      "Epoch 66\n",
      "Accuracy : 0.9404761904761905\n",
      "Epoch 67\n",
      "Accuracy : 0.9404761904761905\n",
      "Epoch 68\n",
      "Accuracy : 0.9404761904761905\n",
      "Epoch 69\n",
      "Accuracy : 0.9404761904761905\n",
      "Epoch 70\n",
      "Accuracy : 0.9404761904761905\n",
      "Epoch 71\n",
      "Accuracy : 0.9404761904761905\n",
      "Epoch 72\n",
      "Accuracy : 0.9404761904761905\n",
      "Epoch 73\n",
      "Accuracy : 0.9428571428571428\n",
      "Epoch 74\n",
      "Accuracy : 0.9428571428571428\n",
      "Epoch 75\n",
      "Accuracy : 0.9428571428571428\n",
      "Epoch 76\n",
      "Accuracy : 0.9428571428571428\n",
      "Epoch 77\n",
      "Accuracy : 0.9428571428571428\n",
      "Epoch 78\n",
      "Accuracy : 0.9428571428571428\n",
      "Epoch 79\n",
      "Accuracy : 0.9476190476190476\n",
      "Epoch 80\n",
      "Accuracy : 0.95\n",
      "Epoch 81\n",
      "Accuracy : 0.95\n",
      "Epoch 82\n",
      "Accuracy : 0.95\n",
      "Epoch 83\n",
      "Accuracy : 0.95\n",
      "Epoch 84\n",
      "Accuracy : 0.95\n",
      "Epoch 85\n",
      "Accuracy : 0.9523809523809523\n",
      "Epoch 86\n",
      "Accuracy : 0.9523809523809523\n",
      "Epoch 87\n",
      "Accuracy : 0.9523809523809523\n",
      "Epoch 88\n",
      "Accuracy : 0.9523809523809523\n",
      "Epoch 89\n",
      "Accuracy : 0.9523809523809523\n",
      "Epoch 90\n",
      "Accuracy : 0.9523809523809523\n",
      "Epoch 91\n",
      "Accuracy : 0.9523809523809523\n",
      "Epoch 92\n",
      "Accuracy : 0.9523809523809523\n",
      "Epoch 93\n",
      "Accuracy : 0.9523809523809523\n",
      "Epoch 94\n",
      "Accuracy : 0.9523809523809523\n",
      "Epoch 95\n",
      "Accuracy : 0.9523809523809523\n",
      "Epoch 96\n",
      "Accuracy : 0.9523809523809523\n",
      "Epoch 97\n",
      "Accuracy : 0.9523809523809523\n",
      "Epoch 98\n",
      "Accuracy : 0.9523809523809523\n",
      "Epoch 99\n",
      "Accuracy : 0.9523809523809523\n",
      "Epoch 100\n",
      "Accuracy : 0.9523809523809523\n",
      "Epoch 101\n",
      "Accuracy : 0.9523809523809523\n",
      "Epoch 102\n",
      "Accuracy : 0.9523809523809523\n",
      "Epoch 103\n",
      "Accuracy : 0.9523809523809523\n",
      "Epoch 104\n",
      "Accuracy : 0.9523809523809523\n",
      "Epoch 105\n",
      "Accuracy : 0.9547619047619048\n",
      "Epoch 106\n",
      "Accuracy : 0.9595238095238096\n",
      "Epoch 107\n",
      "Accuracy : 0.9595238095238096\n",
      "Epoch 108\n",
      "Accuracy : 0.9595238095238096\n",
      "Epoch 109\n",
      "Accuracy : 0.9595238095238096\n",
      "Epoch 110\n",
      "Accuracy : 0.9595238095238096\n",
      "Epoch 111\n",
      "Accuracy : 0.9595238095238096\n",
      "Epoch 112\n",
      "Accuracy : 0.9595238095238096\n",
      "Epoch 113\n",
      "Accuracy : 0.9595238095238096\n",
      "Epoch 114\n",
      "Accuracy : 0.9595238095238096\n",
      "Epoch 115\n",
      "Accuracy : 0.9595238095238096\n",
      "Epoch 116\n",
      "Accuracy : 0.9595238095238096\n",
      "Epoch 117\n",
      "Accuracy : 0.9595238095238096\n",
      "Epoch 118\n",
      "Accuracy : 0.9595238095238096\n",
      "Epoch 119\n",
      "Accuracy : 0.9595238095238096\n",
      "Epoch 120\n",
      "Accuracy : 0.9595238095238096\n",
      "Epoch 121\n",
      "Accuracy : 0.9595238095238096\n",
      "Epoch 122\n",
      "Accuracy : 0.9595238095238096\n",
      "Epoch 123\n",
      "Accuracy : 0.9595238095238096\n",
      "Epoch 124\n",
      "Accuracy : 0.9619047619047619\n",
      "Epoch 125\n",
      "Accuracy : 0.9619047619047619\n",
      "Epoch 126\n",
      "Accuracy : 0.9619047619047619\n",
      "Epoch 127\n",
      "Accuracy : 0.9619047619047619\n",
      "Epoch 128\n",
      "Accuracy : 0.9619047619047619\n",
      "Epoch 129\n",
      "Accuracy : 0.9619047619047619\n",
      "Epoch 130\n",
      "Accuracy : 0.9619047619047619\n",
      "Epoch 131\n",
      "Accuracy : 0.9619047619047619\n",
      "Epoch 132\n",
      "Accuracy : 0.9619047619047619\n",
      "Epoch 133\n",
      "Accuracy : 0.9619047619047619\n",
      "Epoch 134\n",
      "Accuracy : 0.9619047619047619\n",
      "Epoch 135\n",
      "Accuracy : 0.9619047619047619\n",
      "Epoch 136\n",
      "Accuracy : 0.9619047619047619\n",
      "Epoch 137\n",
      "Accuracy : 0.9619047619047619\n",
      "Epoch 138\n",
      "Accuracy : 0.9619047619047619\n",
      "Epoch 139\n",
      "Accuracy : 0.9619047619047619\n",
      "Epoch 140\n",
      "Accuracy : 0.9619047619047619\n",
      "Epoch 141\n",
      "Accuracy : 0.9619047619047619\n",
      "Epoch 142\n",
      "Accuracy : 0.9619047619047619\n",
      "Epoch 143\n",
      "Accuracy : 0.9619047619047619\n",
      "Epoch 144\n",
      "Accuracy : 0.9619047619047619\n",
      "Epoch 145\n",
      "Accuracy : 0.9619047619047619\n",
      "Epoch 146\n",
      "Accuracy : 0.9619047619047619\n",
      "Epoch 147\n",
      "Accuracy : 0.9619047619047619\n",
      "Epoch 148\n",
      "Accuracy : 0.9619047619047619\n",
      "Epoch 149\n",
      "Accuracy : 0.9619047619047619\n",
      "Epoch 150\n",
      "Accuracy : 0.9619047619047619\n",
      "Epoch 151\n",
      "Accuracy : 0.9619047619047619\n",
      "Epoch 152\n",
      "Accuracy : 0.9619047619047619\n",
      "Epoch 153\n",
      "Accuracy : 0.9619047619047619\n",
      "Epoch 154\n",
      "Accuracy : 0.9619047619047619\n",
      "Epoch 155\n",
      "Accuracy : 0.9619047619047619\n",
      "Epoch 156\n",
      "Accuracy : 0.9619047619047619\n",
      "Epoch 157\n",
      "Accuracy : 0.9619047619047619\n",
      "Epoch 158\n",
      "Accuracy : 0.9619047619047619\n",
      "Epoch 159\n",
      "Accuracy : 0.9619047619047619\n",
      "Epoch 160\n",
      "Accuracy : 0.9619047619047619\n",
      "Epoch 161\n",
      "Accuracy : 0.9619047619047619\n",
      "Epoch 162\n",
      "Accuracy : 0.9642857142857143\n",
      "Epoch 163\n",
      "Accuracy : 0.9666666666666667\n",
      "Epoch 164\n",
      "Accuracy : 0.9666666666666667\n",
      "Epoch 165\n",
      "Accuracy : 0.9666666666666667\n",
      "Epoch 166\n",
      "Accuracy : 0.969047619047619\n",
      "Epoch 167\n",
      "Accuracy : 0.969047619047619\n",
      "Epoch 168\n",
      "Accuracy : 0.969047619047619\n",
      "Epoch 169\n",
      "Accuracy : 0.969047619047619\n",
      "Epoch 170\n",
      "Accuracy : 0.969047619047619\n",
      "Epoch 171\n",
      "Accuracy : 0.969047619047619\n",
      "Epoch 172\n",
      "Accuracy : 0.969047619047619\n",
      "Epoch 173\n",
      "Accuracy : 0.969047619047619\n",
      "Epoch 174\n",
      "Accuracy : 0.969047619047619\n",
      "Epoch 175\n",
      "Accuracy : 0.969047619047619\n",
      "Epoch 176\n",
      "Accuracy : 0.969047619047619\n",
      "Epoch 177\n",
      "Accuracy : 0.969047619047619\n",
      "Epoch 178\n",
      "Accuracy : 0.969047619047619\n",
      "Epoch 179\n",
      "Accuracy : 0.969047619047619\n",
      "Epoch 180\n",
      "Accuracy : 0.9714285714285714\n",
      "Epoch 181\n",
      "Accuracy : 0.9714285714285714\n",
      "Epoch 182\n",
      "Accuracy : 0.9714285714285714\n",
      "Epoch 183\n",
      "Accuracy : 0.9714285714285714\n",
      "Epoch 184\n",
      "Accuracy : 0.9714285714285714\n",
      "Epoch 185\n",
      "Accuracy : 0.9714285714285714\n",
      "Epoch 186\n",
      "Accuracy : 0.9714285714285714\n",
      "Epoch 187\n",
      "Accuracy : 0.9714285714285714\n",
      "Epoch 188\n",
      "Accuracy : 0.9714285714285714\n",
      "Epoch 189\n",
      "Accuracy : 0.9714285714285714\n",
      "Epoch 190\n",
      "Accuracy : 0.9714285714285714\n",
      "Epoch 191\n",
      "Accuracy : 0.9714285714285714\n",
      "Epoch 192\n",
      "Accuracy : 0.9714285714285714\n",
      "Epoch 193\n",
      "Accuracy : 0.9714285714285714\n",
      "Epoch 194\n",
      "Accuracy : 0.9714285714285714\n",
      "Epoch 195\n",
      "Accuracy : 0.9714285714285714\n",
      "Epoch 196\n",
      "Accuracy : 0.9714285714285714\n",
      "Epoch 197\n",
      "Accuracy : 0.9714285714285714\n",
      "Epoch 198\n",
      "Accuracy : 0.9714285714285714\n",
      "Epoch 199\n",
      "Accuracy : 0.9714285714285714\n"
     ]
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The above output is the Training Result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct : 129\n",
      "Total : 140\n",
      "정답률 : 0.9214285714285714\n"
     ]
    }
   ],
   "source": [
    "model.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The above output is the Test Result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "현재 설정한 Model의 Architecture는 다음과 같다.  \n",
    "* Layer Num : 4  \n",
    "* Layer Node Num : [256, 96, 48, 7]  \n",
    "* Learning Rate : 0.03  \n",
    "* Epoch Num : 200(Python)  \n",
    "<br>\n",
    "    \n",
    "1) train() 함수를 실행하여 Training Data Set에 대해서 Forward Pass/Backward Pass/Parameter Update를 진행한다.  \n",
    "    먼저 한 개의 Example에 대해서 Forward Pass/Backward Pass를 진행하여 dW, db를 구하고,  \n",
    "    모든 Exaple에 대한 Gradient에 대한 평균을 구한 뒤, 이를 바탕으로 Parameter Update를 진행하여 1 Epoch을 수행한다.  \n",
    "    그리고 이 과정을 지정한 Epoch 수만큼 반복하여 Training을 진행한다.  \n",
    "    \n",
    "2) predict() 함수를 실행하여 학습된 parameter 값을 바탕으로 140개의 Test Data Set에 대해 Forward Pass를 진행, Accuracy를 도출한다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다양한 Hyper Parameter(Learning Rate, Layer Num, Layer Node Num, Parameter Initialization)를 바꿔가면서 실험을 진행했습니다.  \n",
    "그런데, Test Data Set을 만들 때 Training Data Set에 비해서 다소 변칙적인 이미지를 많이 생성하였더니,  \n",
    "해당 데이터들에 대해서는 학습 과정에서 완전히 학습되지 못해, 100%에 가까운 Accuracy를 보이지는 못했습니다.  \n",
    "  \n",
    "이 문제 또한 해결하기 위해 Data Augmentation/L2 Regularization과 같은 다양한 Regularization 방법을 적용해 보려고 하였으나,  \n",
    "이 부분은 구현 상의 어려움과 시간 부족으로 완벽하게 실행해 보지는 못했습니다.  \n",
    "  \n",
    "이에 따라 일반적으로, 다음과 같은 결과를 얻게 되었습니다.  \n",
    "  \n",
    "  * Python: Training Data (Accuracy 약 97%), Test Data (Accuracy 약 92%) \n",
    "\n",
    "이후에 좀 더 다양한 방법을 적용해보면서 모델을 개선시킬 수 있는 방법을 찾아보고자 합니다.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
